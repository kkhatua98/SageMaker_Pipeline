{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\n",
      "<sagemaker.session.Session object at 0x7f53b0152210>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "# sagemaker_session = sagemaker.session.Session(default_bucket = pipeline_output_bucket)\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = \"arn:aws:iam::852619674999:role/service-role/AmazonSageMaker-ExecutionRole-20220427T124311\"\n",
    "\n",
    "print(role)\n",
    "print(sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking configuration parameter values from config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the configurations from config.json file.\n",
    "import json\n",
    "with open(\"config.json\") as file:\n",
    "    build_parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecase = build_parameters[\"usecase\"]\n",
    "\n",
    "## Handling the input location\n",
    "# Default input location\n",
    "# pipeline_input_bucket = f\"{usecase}-input-bucket-{region}\" \n",
    "pipeline_input_bucket = build_parameters[\"input_bucket\"]\n",
    "\n",
    "# Making the output location runtime parameter\n",
    "# pipeline_input_bucket = ParameterString(name = \"PipelineInputBucket\", default_value = pipeline_s3_input_bucket) \n",
    "\n",
    "\n",
    "# Default location for the datasets\n",
    "input_train_data_uri = f\"s3://{pipeline_input_bucket}/churn-bigml-80.csv\"\n",
    "input_test_data_uri = f\"s3://{pipeline_input_bucket}/churn-bigml-20.csv\"\n",
    "input_evaluation_data_uri = f\"s3://{pipeline_input_bucket}/churn-bigml-20.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Parametrizing Data paths\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "train_data = ParameterString(name=\"TrainData\", default_value = input_train_data_uri)\n",
    "test_data = ParameterString(name=\"TestData\", default_value = input_test_data_uri)\n",
    "evaluation_data = ParameterString(name=\"EvaluationData\", default_value = input_evaluation_data_uri)\n",
    "model_given = ParameterString(name=\"ModelGiven\", default_value = \"No\")\n",
    "model_s3_path = ParameterString(name=\"ModelPath\", default_value = \"-\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling the output location\n",
    "# Default output location\n",
    "# pipeline_s3_output_bucket = f\"{usecase}-output-bucket-{region}\" \n",
    "# pipeline_s3_output_bucket = build_parameters[\"output_bucket\"]\n",
    "pipeline_output_bucket = build_parameters[\"output_bucket\"] \n",
    "\n",
    "# Making the output location runtime parameter\n",
    "# pipeline_output_bucket = ParameterString(name = \"PipelineOutputBucket\", default_value = pipeline_s3_output_bucket) \n",
    "sagemaker_session.default_bucket = pipeline_output_bucket\n",
    "\n",
    "# Creating the output bucket if it is not already present\n",
    "s3 = boto3.client('s3')\n",
    "buckets = [dictionary[\"Name\"] for dictionary in s3.list_buckets()['Buckets']]\n",
    "if pipeline_output_bucket not in buckets:\n",
    "    location = {'LocationConstraint': region}\n",
    "    response = s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration = location)\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "pipeline_start_time = strftime(\"%Y%m%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "from sagemaker.workflow import functions\n",
    "\n",
    "# These variables were written thinking that output path can be taken as parameter, yes it can be done,\n",
    "# but not all the pipeline steps accepts pipeline parameter as input, so we had to pick the output path from config\n",
    "# file instead of as parameter\n",
    "# processing_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"ProcessingOutput\"])\n",
    "# evaluation_processing_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"EvaluationProcessingOutput\"])\n",
    "# # hptune_training_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"HPTuneTrainingOutput\"])\n",
    "# hptune_training_output_path = f\"s3://{pipeline_s3_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/HPTuneTrainingOutput\"\n",
    "# evaluation_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"EvaluationOutput\"])\n",
    "\n",
    "\n",
    "processing_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/ProcessingOutput\"\n",
    "evaluation_processing_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/EvaluationProcessingOutput\"\n",
    "# hptune_training_output_path = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", pipeline_start_time, \"HPTuneTrainingOutput\"])\n",
    "hptune_training_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/HPTuneTrainingOutput\"\n",
    "evaluation_output_path = f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/{pipeline_start_time}/EvaluationOutput\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining runtime parameters related to preprocessing\n",
    "# Defining the default location for the parameters\n",
    "input_feature_selection_file_uri = f\"s3://{pipeline_input_bucket}/Feature_Selection.csv\"\n",
    "# preprocessing_code_location_uri = f\"s3://{pipeline_input_bucket}/codes/Training_Preprocessing.py\"\n",
    "\n",
    "# Basic feature selection file path\n",
    "feature_selection_file = ParameterString(name = \"FeatureSelectionFile\", default_value = input_feature_selection_file_uri)\n",
    "# preprocessing_code_location = ParameterString(name = \"ProcessingCodeLocation\", default_value = preprocessing_code_location_uri)\n",
    "\n",
    "# Defining machine types\n",
    "# processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "# processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.m5.4xlarge\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# framework_version = \"0.23-1\"\n",
    "framework_version = build_parameters[\"sklearn_processor_framework_version\"]\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    # framework_version = processing_framework_sklearn_version,\n",
    "    framework_version = framework_version,\n",
    "    instance_type = build_parameters[\"processing_instance_type\"],\n",
    "    instance_count= build_parameters[\"processing_instance_count\"],\n",
    "    base_job_name = f\"{usecase}-preprocessing\",\n",
    "    role=role\n",
    ")\n",
    "\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TuningStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name = \"preprocessing_full_data\",\n",
    "    description = \"Data preprocessing and splitting into train and test set\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source = train_data, destination=\"/opt/ml/processing/input/data\"),  \n",
    "        ProcessingInput(source=feature_selection_file, destination=\"/opt/ml/processing/input/feature_selection\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        # ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination = f\"{processing_output_path}/data/\"),\n",
    "        ProcessingOutput(output_name = \"train\", source=\"/opt/ml/processing/train\", destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])),\n",
    "        # ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\", destination = f\"{processing_output_path}/data/\"),\n",
    "        # ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\", destination = f\"{processing_output_path}/data/\"),\n",
    "        ProcessingOutput(output_name = \"test\", source=\"/opt/ml/processing/test\", destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"data\"])),\n",
    "        # ProcessingOutput(output_name=\"logs\", source=\"/opt/ml/processing/logss\", destination = f\"{processing_output_path}/logs/\")\n",
    "        ProcessingOutput(output_name = \"logs\", source=\"/opt/ml/processing/logss\", destination = sagemaker.workflow.functions.Join(on='/', values = [processing_output_path, \"logs\"])),\n",
    "    ],\n",
    "    # code=\"SageMaker_Pipeline_Component_Codes/Training/Training_Preprocessing.py\",\n",
    "    code = f\"s3://{pipeline_input_bucket}/codes/{build_parameters['processing_code_file_name']}\",\n",
    "    job_arguments = [\"--train_data_location\", \"/opt/ml/processing/input/data\", \"--feature_selection_file_location\", \n",
    "                     \"/opt/ml/processing/input/feature_selection\", \"--target_column\", \"Churn\",\n",
    "                     \"--preprocessed_train_data_location\", \"/opt/ml/processing/train\", \"--preprocessed_test_data_location\", \n",
    "                     \"/opt/ml/processing/test\", \"--log_location\", \"/opt/ml/processing/logss\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor_evaluation = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=build_parameters[\"processing_instance_type\"],\n",
    "    instance_count=build_parameters[\"processing_instance_count\"],\n",
    "    base_job_name=f\"{usecase}-preprocessing-validation\",\n",
    "    role=role\n",
    ")\n",
    "\n",
    "step_process_evaluation = ProcessingStep(\n",
    "    name=\"preprocessing_validation_data\",\n",
    "    # processor=sklearn_processor_evaluation,\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=evaluation_data, destination=\"/opt/ml/processing/input/data\"), \n",
    "        ProcessingInput(source=feature_selection_file, destination=\"/opt/ml/processing/input/feature_selection\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\", destination = sagemaker.workflow.functions.Join(on='/', values = [evaluation_processing_output_path, \"data\"])),\n",
    "        ProcessingOutput(output_name=\"logs\", source=\"/opt/ml/processing/logss\", destination = sagemaker.workflow.functions.Join(on='/', values = [evaluation_processing_output_path, \"logs\"]))\n",
    "    ],\n",
    "    # code=\"SageMaker_Pipeline_Component_Codes/Training/Training_Preprocessing.py\",\n",
    "    code = f\"s3://{pipeline_input_bucket}/codes/{build_parameters['processing_code_file_name']}\",\n",
    "    depends_on = [step_process],\n",
    "    job_arguments = [\"--train_data_location\", \"/opt/ml/processing/input/data\", \"--feature_selection_file_location\", \n",
    "                     \"/opt/ml/processing/input/feature_selection\", \"--target_column\", \"Churn\", \"--stop_split\", \"Y\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_objective_metric_name = build_parameters[\"objective_metric\"]\n",
    "objective_metric_name = ParameterString(name = \"ObjectiveMetric\", default_value = default_objective_metric_name)\n",
    "metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"accuracy:([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "sklearn_image_uri = image_uris.retrieve(framework='sklearn', region=region, version='0.23-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, CategoricalParameter, HyperparameterTuner, WarmStartConfig, WarmStartTypes\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "duplicate_objective_metric_name = ParameterString(name = \"ObjectiveMetric\", default_value = default_objective_metric_name)\n",
    "\n",
    "n_models = build_parameters[\"number_of_models\"]\n",
    "tuning_steps = []\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    if model_details[\"model_type\"] == 'sklearn_model':\n",
    "        estimator = SKLearn(source_dir = f\"s3://{pipeline_input_bucket}/codes/{model_details['model_name']}.tar.gz\", \n",
    "                            entry_point = model_details[\"entry_point\"], \n",
    "                            dependencies = model_details[\"dependencies\"], \n",
    "                            instance_type = model_details[\"instance_type\"], \n",
    "                            framework_version = '0.20.0', \n",
    "                            output_path = f\"{hptune_training_output_path}/{model_details['model_name']}\",\n",
    "                            image_uri = sklearn_image_uri, role = role\n",
    "                            )\n",
    "        \n",
    "        hyperparameters = model_details[\"hyperparameters\"].keys()\n",
    "        hyperparameter_ranges = {}\n",
    "        for hyperparameter in hyperparameters:\n",
    "            if model_details[\"hyperparameters\"][hyperparameter][\"type\"] == \"categorical\":\n",
    "                hyperparameter_ranges[hyperparameter] = CategoricalParameter(model_details[\"hyperparameters\"][hyperparameter][\"values\"])\n",
    "            elif model_details[\"hyperparameters\"][hyperparameter][\"type\"] == \"integer\":\n",
    "                hyperparameter_ranges[hyperparameter] = IntegerParameter(min_value = model_details[\"hyperparameters\"][hyperparameter][\"min_value\"],\n",
    "                                                                         max_value = model_details[\"hyperparameters\"][hyperparameter][\"max_value\"])\n",
    "        \n",
    "        hyperparameter_ranges[\"objective_metric\"] = CategoricalParameter([objective_metric_name, \"anything\"])\n",
    "        tuner = HyperparameterTuner(\n",
    "            estimator,\n",
    "            objective_metric_name,\n",
    "            hyperparameter_ranges,\n",
    "            metric_definitions,\n",
    "            max_jobs=1,\n",
    "            max_parallel_jobs=1,\n",
    "            strategy = model_details[\"tuning_strategy\"],\n",
    "            base_tuning_job_name = model_details[\"model_name\"],\n",
    "            # base_tuning_job_name=f\"Decision_Tree_{strftime('%Y%m%d-%H-%M-%S', gmtime())}\"\n",
    "            )\n",
    "        # print(f\"HPTuning-{model_details['model_name']}\")\n",
    "        step_tuning = TuningStep(\n",
    "            name = f\"hptuning-{model_details['model_name']}\",\n",
    "            tuner = tuner,\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test\": TrainingInput(\n",
    "                    s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        tuning_steps.append(step_tuning)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the best model from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=build_parameters[\"evaluation_instance_type\"],\n",
    "    # accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_best_model_steps = []\n",
    "entry_point='SageMaker_Pipeline_Component_Codes/Training/Evaluation.py',\n",
    "for i in range(n_models):\n",
    "    tuning_step_best_model = Model(image_uri = tuning_steps[i].tuner.estimator.image_uri, \n",
    "                                   source_dir = f\"s3://{pipeline_input_bucket}/codes/evaluation.tar.gz\",\n",
    "                                   # source_dir = build_parameters[\"single_model_evluation_source_dir\"],\n",
    "                                   entry_point = build_parameters[\"single_model_evluation_entry_point\"],\n",
    "                                   model_data = sagemaker.workflow.functions.Join(on='/', values=[hptune_training_output_path, tuning_steps[i].name[9:], tuning_steps[i].properties.BestTrainingJob.TrainingJobName, \"output/model.tar.gz\"]), \n",
    "                                   role = role,\n",
    "                                   sagemaker_session = sagemaker_session\n",
    "                                  )\n",
    "    \n",
    "    step_create_best_model = CreateModelStep(\n",
    "        name = f\"Getting-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        model = tuning_step_best_model,\n",
    "        inputs = inputs\n",
    "    )\n",
    "    \n",
    "    create_best_model_steps.append(step_create_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the best models from each hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    transformer_dt = Transformer(\n",
    "        model_name = create_best_model_steps[i].properties.ModelName,\n",
    "        instance_type = build_parameters[\"evaluation_instance_type\"],\n",
    "        instance_count=1,\n",
    "        output_path=f\"{hptune_training_output_path}/{tuning_steps[i].name[9:]}/BestModel\",\n",
    "        base_transform_job_name = f\"{usecase}-evaluation-{tuning_steps[i].name[9:]}\",\n",
    "        env = {\"MODELS3LOCATION\":create_best_model_steps[i].properties.PrimaryContainer.ModelDataUrl, \n",
    "               \"MODELNAME\":build_parameters[\"model_specifications\"][f\"model{i}\"][\"model_name\"]}\n",
    "    )\n",
    "    evaluation_step = TransformStep(\n",
    "        name=f\"Evaluating-Best-{tuning_steps[i].name[9:]}-Model\",\n",
    "        transformer=transformer_dt,\n",
    "        inputs=TransformInput(data=step_process_evaluation.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri, \n",
    "                              # data_type = \"text/csv\"\n",
    "                             )\n",
    "    )\n",
    "    evaluation_steps.append(evaluation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the best model based on model performance metric on evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for i in range(n_models):\n",
    "    inputs.append(ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[evaluation_steps[i].properties.TransformOutput.S3OutputPath, \"evaluation.csv.out\"]), destination=f\"/opt/ml/processing/input/model{i}\"))\n",
    "# inputs = inputs + [ProcessingInput(sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]), destination=f\"/opt/ml/processing/metrics\")]\n",
    "inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/requirements\")]\n",
    "# inputs = inputs + [ProcessingInput(source = sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_input_bucket, \"codes\", \"preprocessing_requirements.txt\"]), destination = \"/opt/ml/processing/input/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "property_file = PropertyFile(\n",
    "    name=\"property_file\",\n",
    "    output_name=\"property_file\",\n",
    "    path=\"property_file.json\"\n",
    ")\n",
    "\n",
    "step_get_best_model = ProcessingStep(\n",
    "    name = \"Getting-Best-Model\",\n",
    "    description = \"Picking the best model based on the metric value calculated using evaluation data\",\n",
    "    processor = sklearn_processor,\n",
    "    inputs=inputs,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"final_model\", source = \"/opt/ml/processing/final_model\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"logs\", source = \"/opt/ml/processing/logs\", destination = evaluation_output_path),\n",
    "        ProcessingOutput(output_name=\"Metrics\", source = \"/opt/ml/processing/metrics_folder\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "                         # f\"s3://{pipeline_output_bucket}/Training_Pipeline_Output/\")\n",
    "        ProcessingOutput(output_name=\"Feature_Importance\", source = \"/opt/ml/processing/feature_importance\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Confusion_Matrix\", source = \"/opt/ml/processing/confusion_matrix\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"Combined_Dashboard_Data\", source = \"/opt/ml/processing/Combined\", \n",
    "                         destination = functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\"])\n",
    "                        ),\n",
    "        ProcessingOutput(output_name=\"property_file\", source = \"/opt/ml/processing/evaluation\", destination = evaluation_output_path)\n",
    "    ],\n",
    "    # code=\"SageMaker_Pipeline_Component_Codes/Training/Final_Model_Selection.py\",\n",
    "    code = f\"s3://{pipeline_input_bucket}/codes/{build_parameters['get_best_model_code_file_name']}\",\n",
    "    # depends_on = [step_dt_evaluation, step_lr_evaluation],\n",
    "    depends_on = evaluation_steps,\n",
    "    job_arguments = [\"--input_folder\", \"/opt/ml/processing/input\", \"--final_model_location\", \"/opt/ml/processing/final_model\", \n",
    "                     \"--logs_location\", \"/opt/ml/processing/logs\", \n",
    "                     # \"--model_metric_input_location\", \"/opt/ml/processing/metrics\", \n",
    "                     \"--model_metric_input_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Model_Performance_Metrics.csv\"]),\n",
    "                     \"--model_metric_output_location\", \"/opt/ml/processing/metrics_folder\", \"--objective_metric\", objective_metric_name, \n",
    "                     \"--property_file_location\", \"/opt/ml/processing/evaluation\", \n",
    "                     \"--feature_importance_input_file_location\", sagemaker.workflow.functions.Join(on='/', values=[\"s3:/\", pipeline_output_bucket, \"Training_Pipeline_Output\", \"Feature_Importance.csv\"]),\n",
    "                     \"--feature_importance_output_file_location\", \"/opt/ml/processing/feature_importance\"\n",
    "                    ],\n",
    "    property_files=[property_file]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register best model in SageMaker model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_best_model_steps = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    if model_details[\"model_type\"] == 'sklearn_model':\n",
    "        estimator = SKLearn(entry_point = \"\", \n",
    "                            \n",
    "                            instance_type = model_details[\"instance_type\"],\n",
    "                            framework_version = '0.20.0', \n",
    "                            image_uri = sklearn_image_uri,\n",
    "                            \n",
    "                            role = role\n",
    "                            )\n",
    "        register_best_model_step = RegisterModel(name=f\"RegisterBest{model_details['model_name']}Model\", \n",
    "                              estimator = estimator, \n",
    "                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "                              model_data=sagemaker.workflow.functions.Join(on='/', values=[step_get_best_model.properties.ProcessingOutputConfig.Outputs[\"final_model\"].S3Output.S3Uri, \"model.tar.gz\"]),\n",
    "                              content_types=[\"text/csv\"],\n",
    "                              response_types=[\"text/csv\"],\n",
    "                              inference_instances=[model_details[\"instance_type\"]],\n",
    "                              transform_instances=[model_details[\"instance_type\"]],\n",
    "                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "                              image_uri = sklearn_image_uri,\n",
    "                              # approval_status=\"Approved\",\n",
    "                              role=role,\n",
    "                              depends_on = []\n",
    "                             )\n",
    "        register_best_model_steps.append(register_best_model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionEquals\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "condition_steps = []\n",
    "for i in range(n_models):\n",
    "    model_details = build_parameters[\"model_specifications\"][f\"model{i}\"]\n",
    "    condition_equal = ConditionEquals(left = JsonGet(step_name=step_get_best_model.name, \n",
    "                                                   property_file=property_file, \n",
    "                                                   json_path=\"best_model_name\"),\n",
    "                                      right = model_details[\"model_name\"]\n",
    "                                     )\n",
    "    step_cond = ConditionStep(\n",
    "        name=f\"Is-{model_details['model_name']}-Best-Model\",\n",
    "        conditions=[condition_equal],\n",
    "        if_steps = [register_best_model_steps[i]],\n",
    "        )\n",
    "    condition_steps.append(step_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if build_parameters[\"given_model_type\"] == \"sklearn\":\n",
    "#     estimator = SKLearn(entry_point = \"\", \n",
    "                        \n",
    "#                         instance_type = build_parameters[\"scoring_instance_type\"],\n",
    "#                         framework_version = '0.20.0', \n",
    "#                         image_uri = sklearn_image_uri,\n",
    "                        \n",
    "#                         role = role\n",
    "#                         )\n",
    "    \n",
    "#     register_given_model_step = RegisterModel(name=f\"RegisterGivenModel\", \n",
    "#                                              estimator = estimator, \n",
    "#                                              # model_data=step_tuning.get_top_model_s3_uri(top_k=0, s3_bucket=model_path),\n",
    "#                                              model_data=build_parameters[\"given_model_path\"],\n",
    "#                                              content_types=[\"text/csv\"],\n",
    "#                                              response_types=[\"text/csv\"],\n",
    "#                                              inference_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              transform_instances=[build_parameters[\"scoring_instance_type\"]],\n",
    "#                                              model_package_group_name = build_parameters[\"model_package_group_name\"],\n",
    "#                                              image_uri = sklearn_image_uri,\n",
    "#                                              approval_status=\"Approved\",\n",
    "#                                              role=role,\n",
    "#                                              depends_on = []\n",
    "#                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_given_condition = ConditionEquals(left = build_parameters[\"model_given\"],\n",
    "#                                         right = \"No\"\n",
    "#                                        )\n",
    "# step_model_given_cond = ConditionStep(\n",
    "#     name=f\"Is-Model-Given\",\n",
    "#     conditions=[model_given_condition],\n",
    "#     # if_steps = [register_given_model_step],\n",
    "#     if_steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps,\n",
    "#     else_steps = [register_given_model_step]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging the steps inside pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{usecase}-training\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data,\n",
    "        test_data,\n",
    "        evaluation_data,\n",
    "        feature_selection_file,\n",
    "        pipeline_output_bucket,\n",
    "        #model_given,\n",
    "        #model_path,\n",
    "#         pipeline_output_path,\n",
    "        # processing_instance_count,\n",
    "        objective_metric_name,\n",
    "        # processing_code_location,\n",
    "        #training_instance_type,\n",
    "        #evaluation_instance_type,\n",
    "        processing_instance_type,\n",
    "        training_instance_type\n",
    "    ],\n",
    "#     steps=[step_process, step_process_evaluation, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_cond],\n",
    "#     steps=[step_process_evaluation, step_process, step_tuning_dt, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_cond]\n",
    "#     steps=[step_process, step_tuning_dt, step_process_evaluation, step_tuning_lr, step_create_best_dt_model, step_create_best_lr_model, step_dt_evaluation, step_lr_evaluation, step_get_best_model, step_register_best_model]\n",
    "#     steps = [step_cond]\n",
    "    steps = [step_process] + tuning_steps + [step_process_evaluation] + create_best_model_steps + evaluation_steps + [step_get_best_model] + condition_steps\n",
    "#     steps = [step_model_given_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
